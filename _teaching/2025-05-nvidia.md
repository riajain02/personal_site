---
title: "Deep Learning Algorithms Intern"
collection: teaching
type: "Internship"
permalink: /teaching/2025-05-nvidia
venue: "NVIDIA"
date: 2025-05-01
location: "Remote"
---

Worked on distributed, data center-scale LLM inference in Dynamo, NVIDIA's open-source distributed inference framework.

- Architected and implemented asynchronous, distributed speculative decoding. Redesigned LLM forward pass in inference engine (TensorRT-LLM) for multi-process speculation, reducing GPU idle time and enabling asynchronous draft token generation.
- Benchmarked against standard tensor-parallelized speculation workloads, observing reduced inter-token latency.
- Extended existing metrics infrastructure to publish speculative decoding performance metrics for intelligent routing and analysis.

